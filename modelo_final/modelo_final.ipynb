{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado Final: Comparación de Modelos y Ensambles  \n",
    "Implementación de modelos de regresión para predecir **ventas mensuales** en el contexto de retail.  \n",
    "Se pone a prueba la técnica de ensamble, evaluando su desempeño con métricas clave.\n",
    "\n",
    "## **Objetivos**\n",
    "- Implementar modelos individuales y de ensamble.  \n",
    "- Evaluar su desempeño en términos de error y precisión.  \n",
    "- Identificar el modelo óptimo para la predicción de ventas.  \n",
    "- Visualizar la importancia de características y los resultados obtenidos.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y Preparación de Datos  \n",
    "Se importan los datos desde un archivo CSV, eliminando valores nulos y seleccionando las variables relevantes para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos y preparar los datos\n",
    "df = pd.read_csv('datos_retail_transformed.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Seleccionamos las variables numéricas\n",
    "X = df.select_dtypes(include=['number']).drop(columns=['venta_mensual'])  # Variables predictoras\n",
    "y = df['venta_mensual']  # Variable objetivo\n",
    "\n",
    "# Dividimos datos en entrenamiento y prueba (80% entrenamiento y 20% para prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Individuales  \n",
    "En esta sección se entrenan los siguientes modelos:\n",
    "- **Regresión Lineal:** Modelo base para evaluar relaciones lineales.\n",
    "- **Ridge Regression:** Variante de regresión lineal con regularización L2.\n",
    "- **Random Forest:** Algoritmo basado en árboles de decisión, útil para relaciones no lineales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos base que tuvieron mejor desempeño anteriormente\n",
    "\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    \"XGBRegressor\": XGBRegressor(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizacion\n",
    "Se buscara optimizar cada uno de los modelos para luego incluirlo en un ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los parametros que usaremos para GridSearch\n",
    "param_grids = {\n",
    "    'Decision Tree': {'max_depth': [5, 10, 20, None], 'min_samples_split': [2, 5, 10]},\n",
    "    'Random Forest': {'n_estimators': [100, 200, 300], 'max_depth': [5, 10, None]},\n",
    "    \"XGBRegressor\": {'n_estimators': [100, 300, 500], 'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [3, 5, 7]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos GridSearch para cada modelo\n",
    "best_models = {}\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de Modelos  \n",
    "Los modelos se evaluarán utilizando las siguientes métricas:  \n",
    "- **MAE (Mean Absolute Error):** Mide el error medio absoluto.  \n",
    "- **RMSE (Root Mean Squared Error):** Penaliza los errores grandes.  \n",
    "- **R² Score:** Indica qué tan bien el modelo explica la varianza de los datos.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for Decision Tree...\n",
      "Running GridSearchCV for Random Forest...\n",
      "Running GridSearchCV for XGBRegressor...\n",
      "\n",
      "Decision Tree: Best Params: {'max_depth': 20, 'min_samples_split': 5}\n",
      "MAE = 0.0109, RMSE = 0.0243\n",
      "\n",
      "Random Forest: Best Params: {'max_depth': None, 'n_estimators': 200}\n",
      "MAE = 0.0049, RMSE = 0.0132\n",
      "\n",
      "XGBRegressor: Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500}\n",
      "MAE = 0.0061, RMSE = 0.0121\n",
      "\n",
      "Best model: XGBRegressor with RMSE = 0.0121\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"Running GridSearchCV for {name}...\")\n",
    "    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Guaramos el mejor modelo\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Predicciones con mejor modelo\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "    \n",
    "    # Calculamos metricas\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    results[name] = {'Best Params': grid_search.best_params_, 'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "# Imprimimos los resultados\n",
    "for model, score in results.items():\n",
    "    print(f\"\\n{model}: Best Params: {score['Best Params']}\")\n",
    "    print(f\"MAE = {score['MAE']:.4f}, RMSE = {score['RMSE']:.4f}\")\n",
    "\n",
    "# Seleccionamos el mejor modelo con RMSE\n",
    "best_model_name = min(results, key=lambda x: results[x]['RMSE'])\n",
    "print(f\"\\nBest model: {best_model_name} with RMSE = {results[best_model_name]['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Ensamble  \n",
    "Se implementan estrategias de ensamble para mejorar la predicción:\n",
    "- **Voting Regressor:** Promedia las predicciones de los modelos base.\n",
    "- **Stacking Regressor:** Utiliza un metamodelo para combinar predicciones de otros modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos los mejores modelos encontrados en GridSearchCV\n",
    "best_decision_tree = best_models['Decision Tree']\n",
    "best_random_forest = best_models['Random Forest']\n",
    "best_xgb = best_models['XGBRegressor']\n",
    "\n",
    "# Para VotingRegressor, usamos una combinación de los mejores modelos\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('dt', best_decision_tree),\n",
    "    ('rf', best_random_forest),\n",
    "    ('xgb', best_xgb)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voting Regressor:\n",
      "MAE = 0.0055, RMSE = 0.0121\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrenamos Voting Regressor\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Hacemos predicciones con el Voting Regressor\n",
    "y_pred_voting = voting_regressor.predict(X_test)\n",
    "\n",
    "# Calculamos métricas para Voting Regressor\n",
    "mae_voting = mean_absolute_error(y_test, y_pred_voting)\n",
    "rmse_voting = np.sqrt(mean_squared_error(y_test, y_pred_voting))\n",
    "\n",
    "print(f\"\\nVoting Regressor:\")\n",
    "print(f\"MAE = {mae_voting:.4f}, RMSE = {rmse_voting:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacking Regressor:\n",
      "MAE = 0.0052, RMSE = 0.0107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Definimos los modelos base para el Stacking Regressor\n",
    "base_models = [\n",
    "    ('dt', best_decision_tree),\n",
    "    ('rf', best_random_forest),\n",
    "    ('xgb', best_xgb)\n",
    "]\n",
    "\n",
    "# Usamos una regresión lineal como meta-modelo\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "# Creamos el Stacking Regressor\n",
    "stacking_regressor = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "# Entrenamos el Stacking Regressor\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones con Stacking Regressor\n",
    "y_pred_stacking = stacking_regressor.predict(X_test)\n",
    "\n",
    "# Calculamos métricas para Stacking Regressor\n",
    "mae_stacking = mean_absolute_error(y_test, y_pred_stacking)\n",
    "rmse_stacking = np.sqrt(mean_squared_error(y_test, y_pred_stacking))\n",
    "\n",
    "print(f\"\\nStacking Regressor:\")\n",
    "print(f\"MAE = {mae_stacking:.4f}, RMSE = {rmse_stacking:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree: Best Params: {'max_depth': 20, 'min_samples_split': 5}\n",
      "MAE = 0.0109, RMSE = 0.0243\n",
      "\n",
      "Random Forest: Best Params: {'max_depth': None, 'n_estimators': 200}\n",
      "MAE = 0.0049, RMSE = 0.0132\n",
      "\n",
      "XGBRegressor: Best Params: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500}\n",
      "MAE = 0.0061, RMSE = 0.0121\n",
      "\n",
      "Voting Regressor: Best Params: N/A\n",
      "MAE = 0.0055, RMSE = 0.0121\n",
      "\n",
      "Stacking Regressor: Best Params: N/A\n",
      "MAE = 0.0052, RMSE = 0.0107\n",
      "\n",
      "Best model: Stacking Regressor with RMSE = 0.0107\n"
     ]
    }
   ],
   "source": [
    "# Agregamos los resultados de los modelos de ensamble\n",
    "results['Voting Regressor'] = {'Best Params': 'N/A', 'MAE': mae_voting, 'RMSE': rmse_voting}\n",
    "results['Stacking Regressor'] = {'Best Params': 'N/A', 'MAE': mae_stacking, 'RMSE': rmse_stacking}\n",
    "\n",
    "# Imprimimos todos los resultados\n",
    "for model, score in results.items():\n",
    "    print(f\"\\n{model}: Best Params: {score['Best Params']}\")\n",
    "    print(f\"MAE = {score['MAE']:.4f}, RMSE = {score['RMSE']:.4f}\")\n",
    "\n",
    "# Seleccionamos el mejor modelo basado en RMSE\n",
    "best_model_name = min(results, key=lambda x: results[x]['RMSE'])\n",
    "print(f\"\\nBest model: {best_model_name} with RMSE = {results[best_model_name]['RMSE']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('autono-drive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89b95a69b517455424139a566ac4c46bcff0d646d102bc5341370be973cd97c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
